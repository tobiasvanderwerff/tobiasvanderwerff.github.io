---
layout: post
title: "CUDA: using the __CUDA_ARCH__ macro the right way"
tags: tutorial cuda
---

TL;DR: Read [this section of the CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-arch) before using the `__CUDA_ARCH__` macro in any kind of non-trivial way.

Lately, I've become interested in CUDA C++ to explore the magic behind what makes neural networks run fast on GPUs. 
I tend to do a lot of Python programming, so stepping down into the depths of low-level GPU programming has been, well... illuminating.
If there's one thing I've noticed, it's that learning to write high-performance CUDA code is like learning to program in hard mode. One does not simply "learn" CUDA -- in my experience, the best you can hope for is to make sufficient number of mistakes such that at some point, you can avoid common pitfalls/go from being "completely hopeless" to "mildly competent". More often than not, the devil is in the details, and unfortunately the details are all too often easy to miss. [^1]

In this post, I want to highlight a specific detail of CUDA C/C++ that can easily lead to mistakes: the `__CUDA_ARCH__` macro.

First, a bit of context. The `__CUDA_ARCH__` macro is used to write code that behaves differently depending on your GPU architecture. NVIDIA tends to release a new GPU architecture every two years or so, which may provide functionality that previous generations did not, so it's quite useful to have the ability to write architecture-specific code. 

Inside device code (CUDA code), the `__CUDA_ARCH__` macro expands to the compute capability of the GPU that you're compiling for. For example, for an NVIDIA A100, which has compute capability sm80, `__CUDA_ARCH__` will expand to `800`. For a GeForce RTX 4090, which has compute capability sm89, `__CUDA_ARCH__` will expand to `890`. By contrast, inside host code, the value of the macro is undefined. We can use the value of `__CUDA_ARCH__` to conditionally include code that may only work for specific GPU architectures.

%% Here's an example of using `__CUDA_ARCH__`, from the [CUDA Docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/):

As an example, here's a snippet from [Flash Attention](https://github.com/Dao-AILab/flash-attention/blob/478ee666cccbd1b8f63648633003059a8dc6827d/csrc/flash_attn/src/kernel_traits.h#L18) that selects the input type `Element` based on the GPU architecture is at least [Ampere](https://en.wikipedia.org/wiki/Ampere_(microarchitecture) (sm80):

```cpp
template<...>
struct Flash_kernel_traits {

#if defined(__CUDA_ARCH__) &&  __CUDA_ARCH__ >= 800
    using Element = elem_type;
    static constexpr bool Has_cp_async = true;
#else
    using Element = cutlass::half_t;
    static constexpr bool Has_cp_async = false;
#endif

    // ...

}
```

Also note the use of `Has_cp_async`: The `cp_async` [operation](https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async) requires sm80 or higher, so the `Has_cp_async` boolean is a way to specify the correct instructions at compile time by using [preprocessor directives](https://cplusplus.com/doc/tutorial/preprocessor/).

## Undefined behavior

Although the `__CUDA_ARCH__` macro is quite useful, there are various situations where using `__CUDA_ARCH__` actually leads to **undefined behavior**. The CUDA C++ programming guide has an important [section](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-arch) that highlights 4 situations where using `__CUDA_ARCH__` leads to undefined behavior [^2].

Roughly speaking, these are the 4 rules that shouldn't be violated:
1. Setting type signatures for `__global__` functions and/or `__device__` and `__constant__` variables that depend on `__CUDA_ARCH__`
2. Instantiating function templates for `__global__` functions based on `__CUDA_ARCH__`
3. 
4. 

For example, the following code block violates rule no. 2: 

```cpp
#if !defined(__CUDA_ARCH__)
typedef int mytype;
#else
typedef double mytype;
#endif

__global__ void foo(mytype in, // error: foo's type depends on __CUDA_ARCH__
                    mytype *ptr)
{
  *ptr = in;
}
```

What exacty happens when you violate any of these 4 rules? The docs provide the answer:

> The compiler does not guarantee that a diagnostic will be generated for the unsupported uses of __CUDA_ARCH__ described above.

In other words, your code can now exhibit arbitrary behavior, including crashing, producing incorrect results, or even behaving as expected by coincidence. What's worse, the compiler won't even tell you that something is wrong. 

Someone from the GPU Mode discord channel said it best:

> This is probably the scariest kind of thing you can have in C/C++: UB NDR (Undefined Behaviour, No Diagnostic Required)

Scary stuff. If you take away nothing else from the current post, let it be this: read the aforementioned [section](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-arch) of the CUDA programming guide before you find yourself using `__CUDA_ARCH__` in any kind of non-trivial way.

## Case study: torchao FP6 kernel

To get an impression of how things can go wrong, let's look at a case study from [torchao](https://github.com/pytorch/ao), a library for PyTorch native quantization and sparsity for training and inference, to which I have been contributing in the last few months. Specifically, `torchao` includes an integration with [FP6](https://arxiv.org/abs/2401.14112), a high-performance CUDA kernel for 6-bit quantization. Most of the details aren't important, but I just want to highlight a small part of the code. Here's a minified version of what the FP6 code looked like at some point:

```cpp
#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 800

// ...

// template<typename TilingConfig, typename OutputDataType, int EXPONENT, int MANTISSA>
// __global__ void fpx_kernel(const uint4* Weight, const half* Scales,
//                                   const half *B,
//                                   OutputDataType* C,
//                                   const size_t M_Global, const size_t N_Global, const size_t K_Global,
//                                   int Split_K)
// {
//     // Actual CUDA code here ...
// }
//
// template<typename TilingConfig, typename OutputDataType, int EXPONENT, int MANTISSA>
// static void Kernel_Ex(cudaStream_t    stream,
//                       const uint4     *Weight,
//                       const half      *Scales,
//                       const half      *B,
//                       OutputDataType  *C,
//                       const size_t    M_Global,
//                       const size_t    N_Global,
//                       const size_t    K_Global, 
//                       int             Split_K) 
// {
//     // Code here ...
// }
// 
// torch::Tensor fp6_forward_cuda(
//     int64_t         EXPONENT,
//     int64_t         MANTISSA,
//     torch::Tensor   in_feats,
//     torch::Tensor   weights,
//     torch::Tensor   scales,
//     int64_t         splitK=1)
// {
//     // ...
//     
//     constexpr int EXPONENT = 3;
//     constexpr int MANTISSA = 2;
//     
//     auto stream = at::cuda::getCurrentCUDAStream();
//     
//     fpx_kernel<TilingConfig, OutputDataType, EXPONENT, MANTISSA><<<GridDim, BlockDim, SHMEM_SZ, stream>>>
//                     (weights, scales, in_feats, out_feats, M, N, K, Split_K);
//     
//     return out_feats;
// }

template<int EXPONENT, int MANTISSA>
__global__ void fpx_kernel(const uint4* weight, const half* scales,
                           const half* in_feats, half* out_feats,
                           const size_t M, const size_t N, const size_t K)
{
    // Actual CUDA code here ...
}


torch::Tensor fp6_forward_cuda(
    int64_t         EXPONENT,
    int64_t         MANTISSA,
    torch::Tensor   in_feats,
    torch::Tensor   weights,
    torch::Tensor   scales)
{
    // ...
    
    auto stream = at::cuda::getCurrentCUDAStream();
    
    fpx_kernel</*EXPONENT=*/3, /*MANTISSA=*/2><<<GridDim, BlockDim, stream>>>(weights, scales, in_feats, out_feats, M, N, K);
    fpx_kernel<3, 2><<<GridDim, BlockDim, stream>>>(weights, scales, in_feats, out_feats, M, N, K);
    // TODO: choose one of the two above, depending on which one is more clear.
    
    return out_feats;
}

// Define Pytorch binding
TORCH_LIBRARY_IMPL(torchao, CUDA, m) {
  m.impl("torchao::fp6_forward_cuda", &fp6_forward_cuda);
}

#endif
```

Note that during compilation, the `nvcc` compiler makes a distinction between device code (code that runs on the GPU) and host code (code that runs on the CPU). Host code is forwarded to a standard C++ compiler (like `gcc` or `cl`), similar to any regular C++ program. The device code, on the other hand, is compiled by `nvcc`, the NVIDIA CUDA compiler. Afterwards, the device code is embedded into the host object file as a "fat binary".

The idea here is that the FP6 kernel only supports sm80 and higher, so we only include the code for `__CUDA_ARCH__ >= 800`, or when `__CUDA_ARCH__` is undefined (which is the case for host code). This worked fine on sm80 GPUs and higher.

However, when we ran this code on a sm75 GPU, we noticed that it behaved inconsistently. Sometimes, the code would error out (which would be the expected outcome). Other times, it would actually run without errors, while producing garbage output.

If we refer back to the 4 rules I mentioned earlier from the [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-arch), it appears that we are violating rule no. 2:

```
If a `__global__` function template is instantiated and launched from the host, then the function template must be instantiated with the same template arguments irrespective of whether `__CUDA_ARCH__` is defined and regardless of the value of `__CUDA_ARCH__`.
```

In the case of the FP6 code, it would appear that the following is happening when we compile for an sm75 GPU (such as a NVIDIA T4):

1. For the compilation of host code, the preprocessor directive will evaluate to true (since `!defined(__CUDA_ARCH__)` is true for host code), which means `fpx_kernel<3, 2>` is instantiated.
2. For the compilation of device code, the preprocessor directive will evaluate to false (since `__CUDA_ARCH__ < 800`), which means `fpx_kernel<3, 2>` is _not_ instantiated.

This means there is a problematic divergence between the host and device code: the host code includes `__global__` function symbols that the device code does not. What happens as a result is, well, undefined, so this is clearly problematic. [^3]


## Edge cases


```cpp
__global__ my_kernel(int* a) {
#if __CUDA_ARCH__ < 750
    // path A
#else
    // path B
#endif
}
```

One would expect that path A is only taken in device code, when sm<75. However, path A is also taken in _host code_.

---

[^1]: I want to give a shoutout to the awesome GPU Mode [Discord community](https://discord.gg/gpumode) (formerly known as CUDA Mode), which has some of the best [lectures](https://www.youtube.com/@CUDAMODE) on CUDA I've seen anywhere, as well as being a great community of CUDA hackers.
[^2]: Thanks to [Thien Tran](https://github.com/gau-nernst) for pointing this out to me.
[^3]: A possible fix is to use the `#if` directive inside the `fpx_kernel` function body instead of outside of it, such that the function symbols are the same for device and host code. For example, the function body can be left empty for in the case of an unsupported GPU architecture.

## Further reading

 https://leimao.github.io/blog/CUDA-Compilation-Architecture-Macro/


